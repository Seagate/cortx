{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assumed-failing",
   "metadata": {},
   "source": [
    "# Cortx Pytorch Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-detector",
   "metadata": {},
   "source": [
    "This project builds and demonstrates a pytorch integration. The project uses the Cortx framework to store some data. A Pytorch dataloader is created to download the data and use it for a classification task.\n",
    "\n",
    "We use the Medical MNIST dataset from Kaggle available at https://www.kaggle.com/andrewmvd/medical-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coral-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import logging\n",
    "import shutil\n",
    "import threading\n",
    "from botocore.client import Config\n",
    "from matplotlib import pyplot as plt\n",
    "from botocore.exceptions import ClientError\n",
    "from boto3.s3.transfer import TransferConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reliable-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressPercentage(object):\n",
    "    def __init__(self, filename):\n",
    "        self._filename = filename\n",
    "        self._size = float(os.path.getsize(filename))\n",
    "        self._seen_so_far = 0\n",
    "        self._lock = threading.Lock()\n",
    "        \n",
    "    def __call__(self, bytes_amount):\n",
    "        # To simplify, assume this is hooked up to a single filename\n",
    "        with self._lock:\n",
    "            self._seen_so_far += bytes_amount\n",
    "            percentage = (self._seen_so_far / self._size) * 100\n",
    "            sys.stdout.write(\"\\r%s  %s / %s  (%.2f%%)\" %\n",
    "                             (self._filename, self._seen_so_far, \n",
    "                              self._size, percentage))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "def multipart_upload_with_s3(bucket_name, file_path=None, object_name=None):\n",
    "    # Multipart upload (see notes)\n",
    "    config = TransferConfig(multipart_threshold=1024 * 25, max_concurrency=10,\n",
    "                            multipart_chunksize=1024 * 25, use_threads=True)\n",
    "    key_path = 'multipart_files/{}'.format(object_name)\n",
    "    print(bucket_name,file_path,object_name,key_path)\n",
    "    s3_client.upload_file(file_path, bucket_name, key_path,\n",
    "                          ExtraArgs={'ACL': 'public-read', \n",
    "                                     'ContentType': 'text/pdf'},\n",
    "                          Config=config, Callback=ProgressPercentage(file_path))\n",
    "    \n",
    "def multipart_download_with_s3(bucket_name, file_path=None, object_name=None):\n",
    "    config = TransferConfig(multipart_threshold=1024 * 25, max_concurrency=10,\n",
    "                            multipart_chunksize=1024 * 25, use_threads=True)\n",
    "    temp_file = os.path.dirname(__file__)\n",
    "    s3_resource.Object(bucket_name, \n",
    "                       object_name\n",
    "                       ).download_file(file_path, Config=config,\n",
    "                                       Callback=ProgressPercentage(temp_file))\n",
    "\n",
    "\"\"\"Functions for buckets operation\"\"\"\n",
    "def create_bucket_op(bucket_name, region):\n",
    "    if region is None:\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "    else:\n",
    "        location = {'LocationConstraint': region}\n",
    "        s3_client.create_bucket(Bucket=bucket_name, \n",
    "                                CreateBucketConfiguration=location)\n",
    "\n",
    "def list_bucket_op(bucket_name, region, operation):\n",
    "    buckets = s3_client.list_buckets()\n",
    "    if buckets['Buckets']:\n",
    "        for bucket in buckets['Buckets']:\n",
    "            print(bucket)\n",
    "            return True\n",
    "    else:\n",
    "        logging.error('unknown bucket operation')\n",
    "        return False\n",
    "    \n",
    "def bucket_operation(bucket_name, region=None, operation='list'):\n",
    "    try:\n",
    "        if operation == 'delete':\n",
    "            s3_client.delete_bucket(Bucket=bucket_name)\n",
    "        elif operation == 'create':\n",
    "            create_bucket_op(bucket_name, region)\n",
    "        elif operation == 'list':\n",
    "            return list_bucket_op(bucket_name, region, operation)\n",
    "        else:\n",
    "            logging.error('unknown bucket operation')\n",
    "            return False\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\"\"\"Functions for objects operation\"\"\"\n",
    "def list_object_op(bucket_name):\n",
    "     s3_objects = s3_client.list_objects_v2(Bucket=bucket_name)\n",
    "     if s3_objects.get('Contents'):\n",
    "         for obj in s3_objects['Contents']:\n",
    "             print(obj)\n",
    "\n",
    "def delete_object_op(bucket_name, object_name, operation):\n",
    "    if not object_name:\n",
    "        logging.error('object_name missing for {}'.format(operation))\n",
    "        return False  \n",
    "    s3_client.delete_object(Bucket=bucket_name, Key=object_name)\n",
    "    return True\n",
    "\n",
    "def upload_download_object_op(bucket_name, object_name, file_path, operation):\n",
    "    if not file_path or not object_name:\n",
    "        logging.error('file_path and/or object_name missing for upload')\n",
    "        return False\n",
    "    if operation == 'upload':\n",
    "        multipart_upload_with_s3(bucket_name=bucket_name, file_path=file_path,\n",
    "                                 object_name=object_name)\n",
    "    else:\n",
    "        multipart_download_with_s3(bucket_name=bucket_name, file_path=file_path,\n",
    "                                   object_name=object_name)\n",
    "    return True\n",
    "\n",
    "def object_operation(bucket_name=None, object_name=None, file_path=None,\n",
    "                     operation='list'):                                                             \n",
    "    try:\n",
    "        if not bucket_name:\n",
    "            logging.error('The bucket name %s is missing for %s operation!'\n",
    "                          % (bucket_name, operation))\n",
    "            return False\n",
    "        if operation == 'list':\n",
    "            list_object_op(bucket_name)\n",
    "        elif operation == 'delete':\n",
    "            return delete_object_op(bucket_name, object_name, operation)\n",
    "        elif operation == 'upload' or operation == 'download':\n",
    "            return upload_download_object_op(bucket_name, object_name,\n",
    "                                             file_path, operation)      \n",
    "        else:\n",
    "            logging.error('unknown object operation')\n",
    "            return False\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\"\"\"Functions for files operation\"\"\"\n",
    "def list_op_file(bucket_name):\n",
    "    current_bucket = s3_resource.Bucket(bucket_name)\n",
    "    print('The files in bucket %s:\\n' % (bucket_name))\n",
    "    for obj in current_bucket.objects.all():\n",
    "        print(obj.meta.data) \n",
    "        \n",
    "    return True\n",
    "\n",
    "def delete_op_file(bucket_name, file_name, operation):\n",
    "    if not file_name:\n",
    "        logging.error('The file name %s is missing for%s operation!' \n",
    "                      % (file_name, operation))\n",
    "        return False\n",
    "    s3_client.delete_object(Bucket=bucket_name, Key=file_name)\n",
    "    return True\n",
    "    \n",
    "def upload_download_op_file(bucket_name, file_name, file_location,\n",
    "                            region, operation):\n",
    "    if not file_location:\n",
    "        logging.error('The file location %d is missing for %s operation!'\n",
    "                      % (file_location, operation))\n",
    "        return False\n",
    "    if operation == 'download':\n",
    "        s3_resource.Bucket(bucket_name).download_file(file_name, file_location)\n",
    "    elif operation == 'upload' and region is None:\n",
    "        s3_resource.Bucket(bucket_name).upload_file(file_location, file_name)\n",
    "    else:\n",
    "         location = {'LocationConstraint': region}\n",
    "         s3_resource.Bucket(bucket_name\n",
    "                            ).upload_file(file_location, file_name,\n",
    "                                          CreateBucketConfiguration=location) \n",
    "    return True\n",
    "    \n",
    "def file_operation(bucket_name=None, file_name=None, file_location=None, \n",
    "                   region=None, operation='list'):\n",
    "    if not bucket_name:\n",
    "        logging.error('The bucket name is %s missing!' % (bucket_name))\n",
    "        return False \n",
    "    try:\n",
    "        if operation == 'list':\n",
    "            return list_op_file(bucket_name)\n",
    "        elif operation == 'delete':\n",
    "            return delete_op_file(bucket_name, file_name, operation)  \n",
    "        elif operation == 'upload' or operation == 'download':\n",
    "            return upload_download_op_file(bucket_name, file_name, \n",
    "                                           file_location, region, operation)\n",
    "        else:\n",
    "            logging.error('unknown file operation')\n",
    "            return False  \n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "typical-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_POINT_URL = 'http://uvo17qqh4jn92xmchpj.vm.cld.sr'\n",
    "A_KEY = 'AKIAtEpiGWUcQIelPRlD1Pi6xQ'\n",
    "S_KEY = 'YNV6xS8lXnCTGSy1x2vGkmGnmdJbZSapNXaSaRhK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exempt-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resource = boto3.resource('s3', endpoint_url=END_POINT_URL,\n",
    "                             aws_access_key_id=A_KEY,\n",
    "                             aws_secret_access_key=S_KEY,\n",
    "                             config=Config(signature_version='s3v4'),\n",
    "                             region_name='US')\n",
    "\n",
    "s3_client = boto3.client('s3', endpoint_url=END_POINT_URL,\n",
    "                         aws_access_key_id=A_KEY,\n",
    "                         aws_secret_access_key=S_KEY,\n",
    "                         config=Config(signature_version='s3v4'),\n",
    "                         region_name='US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "touched-apache",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files in bucket headct:\n",
      "\n",
      "{'Key': '003332.jpeg', 'LastModified': datetime.datetime(2021, 4, 27, 23, 50, 34, tzinfo=tzutc()), 'ETag': '\"520d8ea45f112a33f5f97d39b8df9e79\"', 'Size': 1045, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '003424.jpeg', 'LastModified': datetime.datetime(2021, 4, 27, 23, 50, 32, tzinfo=tzutc()), 'ETag': '\"b41954570a6f85f5e42affa9740791a2\"', 'Size': 1344, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '003904.jpeg', 'LastModified': datetime.datetime(2021, 4, 27, 23, 50, 37, tzinfo=tzutc()), 'ETag': '\"71122c5c904fb9c58a780c9eb61ede41\"', 'Size': 1219, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '004277.jpeg', 'LastModified': datetime.datetime(2021, 4, 27, 23, 50, 35, tzinfo=tzutc()), 'ETag': '\"5576cf5a362b5365edbfa104357a2318\"', 'Size': 1260, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '005383.jpeg', 'LastModified': datetime.datetime(2021, 4, 27, 23, 50, 30, tzinfo=tzutc()), 'ETag': '\"89d1b5efa6402fd18703f29f2d794472\"', 'Size': 947, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '006537.jpeg', 'LastModified': datetime.datetime(2021, 4, 27, 23, 50, 32, tzinfo=tzutc()), 'ETag': '\"21b28c052d1cdcc63f36717c61153eb6\"', 'Size': 825, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '007914.jpeg', 'LastModified': datetime.datetime(2021, 4, 27, 23, 50, 36, tzinfo=tzutc()), 'ETag': '\"95e5bfc482bf37cc7fc1c538d7e8a8a7\"', 'Size': 1296, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '008405.jpeg', 'LastModified': datetime.datetime(2021, 4, 27, 23, 50, 31, tzinfo=tzutc()), 'ETag': '\"a40712fc2488fee86a691259fa270b97\"', 'Size': 1465, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '009181.jpeg', 'LastModified': datetime.datetime(2021, 4, 27, 23, 50, 33, tzinfo=tzutc()), 'ETag': '\"ab5239dd215ac334c43d41f202d2333b\"', 'Size': 1182, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '009386.jpeg', 'LastModified': datetime.datetime(2021, 4, 27, 23, 50, 34, tzinfo=tzutc()), 'ETag': '\"2e2615676e5167d6df219bdd0f5cb3a5\"', 'Size': 1021, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_op_file('headct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-elevation",
   "metadata": {},
   "source": [
    "## Create MEDMINST data buckets on Cortx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sufficient-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['abdomenct', 'breastmri', 'chestct', 'cxr', 'hand', 'headct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "occupational-squad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: The bucket you tried to create already exists, and you own it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created bucket for class: abdomenct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: The bucket you tried to create already exists, and you own it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created bucket for class: breastmri\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: The bucket you tried to create already exists, and you own it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created bucket for class: chestct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: The bucket you tried to create already exists, and you own it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created bucket for class: cxr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: The bucket you tried to create already exists, and you own it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created bucket for class: hand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: The bucket you tried to create already exists, and you own it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created bucket for class: headct\n"
     ]
    }
   ],
   "source": [
    "for class_name in class_names:\n",
    "    bucket_operation(class_name, None, 'create')\n",
    "    print('created bucket for class: {}'.format(class_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wired-duncan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'abdomenct', 'CreationDate': datetime.datetime(2021, 4, 27, 23, 2, 19, tzinfo=tzutc())}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_operation(None, None, operation='list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "involved-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_path = '/home/dreamchild/sgcortex/data/medmnist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "collective-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_class_dirs = os.listdir(dataset_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aggregate-bachelor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hand', 'CXR', 'AbdomenCT', 'BreastMRI', 'HeadCT', 'ChestCT']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_class_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adopted-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_class_dir_files = {}\n",
    "\n",
    "for class_dir in local_class_dirs:\n",
    "    class_dir_files = []\n",
    "    for file in os.listdir(dataset_base_path + class_dir)[:10]:\n",
    "        class_dir_files.append(file)\n",
    "    local_class_dir_files[class_dir] = class_dir_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sunrise-truth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hand': ['005383.jpeg',\n",
       "  '008405.jpeg',\n",
       "  '003424.jpeg',\n",
       "  '006537.jpeg',\n",
       "  '009181.jpeg',\n",
       "  '009386.jpeg',\n",
       "  '003332.jpeg',\n",
       "  '004277.jpeg',\n",
       "  '007914.jpeg',\n",
       "  '003904.jpeg'],\n",
       " 'CXR': ['005383.jpeg',\n",
       "  '008405.jpeg',\n",
       "  '003424.jpeg',\n",
       "  '006537.jpeg',\n",
       "  '009181.jpeg',\n",
       "  '009386.jpeg',\n",
       "  '003332.jpeg',\n",
       "  '004277.jpeg',\n",
       "  '007914.jpeg',\n",
       "  '003904.jpeg'],\n",
       " 'AbdomenCT': ['005383.jpeg',\n",
       "  '008405.jpeg',\n",
       "  '003424.jpeg',\n",
       "  '006537.jpeg',\n",
       "  '009181.jpeg',\n",
       "  '009386.jpeg',\n",
       "  '003332.jpeg',\n",
       "  '004277.jpeg',\n",
       "  '007914.jpeg',\n",
       "  '003904.jpeg'],\n",
       " 'BreastMRI': ['005383.jpeg',\n",
       "  '008405.jpeg',\n",
       "  '003424.jpeg',\n",
       "  '006537.jpeg',\n",
       "  '003332.jpeg',\n",
       "  '004277.jpeg',\n",
       "  '007914.jpeg',\n",
       "  '003904.jpeg',\n",
       "  '000689.jpeg',\n",
       "  '005112.jpeg'],\n",
       " 'HeadCT': ['005383.jpeg',\n",
       "  '008405.jpeg',\n",
       "  '003424.jpeg',\n",
       "  '006537.jpeg',\n",
       "  '009181.jpeg',\n",
       "  '009386.jpeg',\n",
       "  '003332.jpeg',\n",
       "  '004277.jpeg',\n",
       "  '007914.jpeg',\n",
       "  '003904.jpeg'],\n",
       " 'ChestCT': ['005383.jpeg',\n",
       "  '008405.jpeg',\n",
       "  '003424.jpeg',\n",
       "  '006537.jpeg',\n",
       "  '009181.jpeg',\n",
       "  '009386.jpeg',\n",
       "  '003332.jpeg',\n",
       "  '004277.jpeg',\n",
       "  '007914.jpeg',\n",
       "  '003904.jpeg']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_class_dir_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-owner",
   "metadata": {},
   "source": [
    "### Upload (10) files from each category into cortx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "protecting-hostel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/Hand/005383.jpeg to bucket hand\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/Hand/008405.jpeg to bucket hand\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/Hand/003424.jpeg to bucket hand\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/Hand/006537.jpeg to bucket hand\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/Hand/009181.jpeg to bucket hand\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/Hand/009386.jpeg to bucket hand\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/Hand/003332.jpeg to bucket hand\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/Hand/004277.jpeg to bucket hand\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/Hand/007914.jpeg to bucket hand\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/Hand/003904.jpeg to bucket hand\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/CXR/005383.jpeg to bucket cxr\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/CXR/008405.jpeg to bucket cxr\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/CXR/003424.jpeg to bucket cxr\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/CXR/006537.jpeg to bucket cxr\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/CXR/009181.jpeg to bucket cxr\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/CXR/009386.jpeg to bucket cxr\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/CXR/003332.jpeg to bucket cxr\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/CXR/004277.jpeg to bucket cxr\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/CXR/007914.jpeg to bucket cxr\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/CXR/003904.jpeg to bucket cxr\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/AbdomenCT/005383.jpeg to bucket abdomenct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/AbdomenCT/008405.jpeg to bucket abdomenct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/AbdomenCT/003424.jpeg to bucket abdomenct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/AbdomenCT/006537.jpeg to bucket abdomenct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/AbdomenCT/009181.jpeg to bucket abdomenct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/AbdomenCT/009386.jpeg to bucket abdomenct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/AbdomenCT/003332.jpeg to bucket abdomenct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/AbdomenCT/004277.jpeg to bucket abdomenct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/AbdomenCT/007914.jpeg to bucket abdomenct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/AbdomenCT/003904.jpeg to bucket abdomenct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/BreastMRI/005383.jpeg to bucket breastmri\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/BreastMRI/008405.jpeg to bucket breastmri\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/BreastMRI/003424.jpeg to bucket breastmri\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/BreastMRI/006537.jpeg to bucket breastmri\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/BreastMRI/003332.jpeg to bucket breastmri\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/BreastMRI/004277.jpeg to bucket breastmri\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/BreastMRI/007914.jpeg to bucket breastmri\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/BreastMRI/003904.jpeg to bucket breastmri\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/BreastMRI/000689.jpeg to bucket breastmri\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/BreastMRI/005112.jpeg to bucket breastmri\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/HeadCT/005383.jpeg to bucket headct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/HeadCT/008405.jpeg to bucket headct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/HeadCT/003424.jpeg to bucket headct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/HeadCT/006537.jpeg to bucket headct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/HeadCT/009181.jpeg to bucket headct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/HeadCT/009386.jpeg to bucket headct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/HeadCT/003332.jpeg to bucket headct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/HeadCT/004277.jpeg to bucket headct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/HeadCT/007914.jpeg to bucket headct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/HeadCT/003904.jpeg to bucket headct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/ChestCT/005383.jpeg to bucket chestct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/ChestCT/008405.jpeg to bucket chestct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/ChestCT/003424.jpeg to bucket chestct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/ChestCT/006537.jpeg to bucket chestct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/ChestCT/009181.jpeg to bucket chestct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/ChestCT/009386.jpeg to bucket chestct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/ChestCT/003332.jpeg to bucket chestct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/ChestCT/004277.jpeg to bucket chestct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/ChestCT/007914.jpeg to bucket chestct\n",
      "uploaded file /home/dreamchild/sgcortex/data/medmnist/ChestCT/003904.jpeg to bucket chestct\n"
     ]
    }
   ],
   "source": [
    "for k in local_class_dir_files.keys():\n",
    "    for file in local_class_dir_files[k]:\n",
    "        upload_file_path = dataset_base_path + k + '/' + file\n",
    "        file_operation(k.lower(), file, upload_file_path, None, 'upload')\n",
    "        print('uploaded file {} to bucket {}'.format(upload_file_path, k.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governing-contributor",
   "metadata": {},
   "source": [
    "### Setup Pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "skilled-chart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files in bucket hand:\n",
      "\n",
      "{'Key': '003332.jpeg', 'LastModified': datetime.datetime(2021, 4, 28, 22, 7, tzinfo=tzutc()), 'ETag': '\"7aab34fe6035251e8bd656c28ce96ef6\"', 'Size': 1357, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '003424.jpeg', 'LastModified': datetime.datetime(2021, 4, 28, 22, 6, 57, tzinfo=tzutc()), 'ETag': '\"f9207043d0836eac5d7a282a4fffa152\"', 'Size': 1555, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '003904.jpeg', 'LastModified': datetime.datetime(2021, 4, 28, 22, 7, 2, tzinfo=tzutc()), 'ETag': '\"1e1331b574296a60aff80e0247ba9f7b\"', 'Size': 1824, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '004277.jpeg', 'LastModified': datetime.datetime(2021, 4, 28, 22, 7, 1, tzinfo=tzutc()), 'ETag': '\"3d2ac44ab6f03243946d244a8888fc9b\"', 'Size': 1866, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '005383.jpeg', 'LastModified': datetime.datetime(2021, 4, 28, 22, 6, 55, tzinfo=tzutc()), 'ETag': '\"5177b49310a69f0418e49452c8dd1633\"', 'Size': 1646, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '006537.jpeg', 'LastModified': datetime.datetime(2021, 4, 28, 22, 6, 58, tzinfo=tzutc()), 'ETag': '\"74e44ae9d7662e286cd7da3892531433\"', 'Size': 1723, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '007914.jpeg', 'LastModified': datetime.datetime(2021, 4, 28, 22, 7, 1, tzinfo=tzutc()), 'ETag': '\"f4affc95e2a6aa01655dfd698d2e9734\"', 'Size': 1528, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '008405.jpeg', 'LastModified': datetime.datetime(2021, 4, 28, 22, 6, 56, tzinfo=tzutc()), 'ETag': '\"7da7c57818969c60ae6f40472fd4a969\"', 'Size': 1615, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '009181.jpeg', 'LastModified': datetime.datetime(2021, 4, 28, 22, 6, 58, tzinfo=tzutc()), 'ETag': '\"afd156f564ba0ec79e117c79ae498e50\"', 'Size': 1784, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n",
      "{'Key': '009386.jpeg', 'LastModified': datetime.datetime(2021, 4, 28, 22, 6, 59, tzinfo=tzutc()), 'ETag': '\"d8cbd922ac4e4fb587bf6bfcbb6688fc\"', 'Size': 1620, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': 'S3user', 'ID': 'e500ea6b45f64f068ab001b7f1fdfc57ed2faed247474d81b66f69a6233727c8'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_operation('hand',operation='list')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-flour",
   "metadata": {},
   "source": [
    "## CortxDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-aircraft",
   "metadata": {},
   "source": [
    "Download datasets from remote CORTX instance. Each class file is stored in a different bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "anonymous-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES=True\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "public-liquid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8693079292631422"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "knowing-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((64,64)),    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225] )\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-india",
   "metadata": {},
   "source": [
    "## Introducing the CORTX Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-carol",
   "metadata": {},
   "source": [
    "The CORTX Dataloader downloads datasets from your CORTX instance. It automatically sorts your data into a training and testing set using an 80 percent split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "historical-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CortxDataLoader:\n",
    "    \n",
    "    def __init__(self, endpoint = None, accessKey = None, serviceKey = None, class_buckets = [], batch_size = 16):\n",
    "        self.endpoint = endpoint\n",
    "        self.accessKey = accessKey\n",
    "        self.serviceKey = serviceKey\n",
    "        self.class_buckets = class_buckets\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def download_dataset(self, destination_dir = None):\n",
    "        for bucket in self.class_buckets:\n",
    "            s3_bucket = s3_resource.Bucket(bucket)\n",
    "            print('Downloading files for class bucket: {}'.format(bucket))\n",
    "            #files_in_bucket = s3_resource.Bucket(bucket).objects.all()\n",
    "            folders = ['train','validation','test']\n",
    "            current_bucket_objects = s3_bucket.objects.all()\n",
    "            for fileObj in current_bucket_objects:       \n",
    "                rand = random.random()\n",
    "                filename = fileObj.key\n",
    "                if not os.path.exists('/home/dreamchild/sgcortex/data/medmnist_dl/train/' + bucket):                    \n",
    "                    os.makedirs('/home/dreamchild/sgcortex/data/medmnist_dl/train/' + bucket)\n",
    "                    os.makedirs('/home/dreamchild/sgcortex/data/medmnist_dl/val/' + bucket)\n",
    "                folder = \"train\" if rand < 0.8 else \"val\"\n",
    "                with open('/home/dreamchild/sgcortex/data/medmnist_dl/'+ folder + '/' + bucket + '/' + filename, 'wb') as f:\n",
    "                    s3_bucket.download_fileobj(filename, f)\n",
    "                    print('downloaded file: {}'.format(filename))\n",
    "\n",
    "    def train_data_loader(self):\n",
    "        train_data_path = '/home/dreamchild/sgcortex/data/medmnist_dl/train/'\n",
    "        train_data = torchvision.datasets.ImageFolder(root=train_data_path,transform=img_transforms)\n",
    "        train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=self.batch_size)\n",
    "        return train_data_loader\n",
    "    \n",
    "    def test_data_loader(self):\n",
    "        test_data_path = '/home/dreamchild/sgcortex/data/medmnist_dl/test/'\n",
    "        test_data = torchvision.datasets.ImageFolder(root=test_data_path,transform=img_transforms)\n",
    "        test_data_loader = torch.utils.data.DataLoader(test_data, batch_size = self.batch_size)\n",
    "        return test_data_loader\n",
    "    \n",
    "    def val_data_loader(self):\n",
    "        val_data_path = '/home/dreamchild/sgcortex/data/medmnist_dl/val/'\n",
    "        val_data = torchvision.datasets.ImageFolder(root=val_data_path,transform=img_transforms)\n",
    "        val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=self.batch_size)\n",
    "        return val_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-wednesday",
   "metadata": {},
   "source": [
    "Initialize a new CortxDataLoader with different buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "quiet-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "cortx_loader = CortxDataLoader(class_buckets=['hand','breastmri','chestct','abdomenct','cxr','headct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-commitment",
   "metadata": {},
   "source": [
    "Download data to a local destination folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "recreational-wealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files for class bucket: hand\n",
      "downloaded file: 003332.jpeg\n",
      "downloaded file: 003424.jpeg\n",
      "downloaded file: 003904.jpeg\n",
      "downloaded file: 004277.jpeg\n",
      "downloaded file: 005383.jpeg\n",
      "downloaded file: 006537.jpeg\n",
      "downloaded file: 007914.jpeg\n",
      "downloaded file: 008405.jpeg\n",
      "downloaded file: 009181.jpeg\n",
      "downloaded file: 009386.jpeg\n",
      "Downloading files for class bucket: breastmri\n",
      "downloaded file: 000689.jpeg\n",
      "downloaded file: 003332.jpeg\n",
      "downloaded file: 003424.jpeg\n",
      "downloaded file: 003904.jpeg\n",
      "downloaded file: 004277.jpeg\n",
      "downloaded file: 005112.jpeg\n",
      "downloaded file: 005383.jpeg\n",
      "downloaded file: 006537.jpeg\n",
      "downloaded file: 007914.jpeg\n",
      "downloaded file: 008405.jpeg\n",
      "Downloading files for class bucket: chestct\n",
      "downloaded file: 003332.jpeg\n",
      "downloaded file: 003424.jpeg\n",
      "downloaded file: 003904.jpeg\n",
      "downloaded file: 004277.jpeg\n",
      "downloaded file: 005383.jpeg\n",
      "downloaded file: 006537.jpeg\n",
      "downloaded file: 007914.jpeg\n",
      "downloaded file: 008405.jpeg\n",
      "downloaded file: 009181.jpeg\n",
      "downloaded file: 009386.jpeg\n",
      "Downloading files for class bucket: abdomenct\n",
      "downloaded file: 003332.jpeg\n",
      "downloaded file: 003424.jpeg\n",
      "downloaded file: 003904.jpeg\n",
      "downloaded file: 004277.jpeg\n",
      "downloaded file: 005383.jpeg\n",
      "downloaded file: 006537.jpeg\n",
      "downloaded file: 007914.jpeg\n",
      "downloaded file: 008405.jpeg\n",
      "downloaded file: 009181.jpeg\n",
      "downloaded file: 009386.jpeg\n",
      "Downloading files for class bucket: cxr\n",
      "downloaded file: 003332.jpeg\n",
      "downloaded file: 003424.jpeg\n",
      "downloaded file: 003904.jpeg\n",
      "downloaded file: 004277.jpeg\n",
      "downloaded file: 005383.jpeg\n",
      "downloaded file: 006537.jpeg\n",
      "downloaded file: 007914.jpeg\n",
      "downloaded file: 008405.jpeg\n",
      "downloaded file: 009181.jpeg\n",
      "downloaded file: 009386.jpeg\n",
      "Downloading files for class bucket: headct\n",
      "downloaded file: 003332.jpeg\n",
      "downloaded file: 003424.jpeg\n",
      "downloaded file: 003904.jpeg\n",
      "downloaded file: 004277.jpeg\n",
      "downloaded file: 005383.jpeg\n",
      "downloaded file: 006537.jpeg\n",
      "downloaded file: 007914.jpeg\n",
      "downloaded file: 008405.jpeg\n",
      "downloaded file: 009181.jpeg\n",
      "downloaded file: 009386.jpeg\n"
     ]
    }
   ],
   "source": [
    "cortx_loader.download_dataset('/home/dreamchild/sgcortex/data/medmnist-dl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-breast",
   "metadata": {},
   "source": [
    "## Setup Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-influence",
   "metadata": {},
   "source": [
    "A simple Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "identified-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(12288, 84)\n",
    "        self.fc2 = nn.Linear(84, 50)\n",
    "        self.fc3 = nn.Linear(50,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 12288)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "protecting-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplenet = SimpleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-quilt",
   "metadata": {},
   "source": [
    "Setup the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "pursuant-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(simplenet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-person",
   "metadata": {},
   "source": [
    "Use GPU if available else use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "independent-active",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (fc1): Linear(in_features=12288, out_features=84, bias=True)\n",
       "  (fc2): Linear(in_features=84, out_features=50, bias=True)\n",
       "  (fc3): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "simplenet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "opening-stuart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-engineering",
   "metadata": {},
   "source": [
    "Setup training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "elder-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-arena",
   "metadata": {},
   "source": [
    "## Run the training using the Cortx Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(simplenet, optimizer,torch.nn.CrossEntropyLoss(), cortx_loader.train_data_loader(), cortx_loader.val_data_loader(), epochs=5, device=\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
