<img src="https://github.com/Seagate/cortx/blob/main/doc/images/cortx-logo.png">

# CORTX S3 - Label Studio Integration

### CORTX integration with world's most trusted and open-source data annotation tool, Label Studio. Feel the buzz of heavy loads of data annotations on CORTX S3 data storage systems. 

<img src="https://github.com/vilaksh01/cortx/blob/main/doc/integrations/label-studioAPI/Images/Cortx.jpg">

### Change the way the world does by connecting CORTX™— Seagate’s open-source object storage software — with the tools and platforms that underpin the data revolution.

Storing and managing data had never been easy and with flourish of AI, deep learning we have generated paramounts of data called Big Data.

<pre>
The ideal big data storage system would allow storage of a virtually unlimited amount of data, cope both with high rates of random write and read access, flexibly and efficiently deal with a range of different data models, support both structured and unstructured data, and for privacy reasons, only work on encrypted data. Obviously, all these needs cannot be fully satisfied.
</pre>
 
## Background / Overview of the project
<img src="https://github.com/vilaksh01/cortx/blob/main/doc/integrations/label-studioAPI/Images/page3.jpg">

## The Problem

### What we want to solve?

The problem statement is from my personal story, I was working on a wildlife conservation project to save animals from poaching and using biometric sensors to monitor body vitals, with tremendous hardwork I collected enough amount of data, not an easy task- I had to daily meet park rangers to get data but my negligence ruined all my hardwork after fatal HDD crash. Such huge loss in terms of data and time, the project could not be completed yet, due to it. But this time I will ensure that the data generated by the data revolution can be harnessed to the full extent of its potential using CORTX S3 data services which are scalable, reliable and efficient.

### Hypothesis
Integrating Label Studio, an open source data annotation tool widely used by global AI brands. Integrating, S3 services for result storing and data retrieval for data analytics and other data related tasks. Label Studio is compatible with cloud services in it's latest version, so it is good and compatible to integrate Cortx S3 with it.

<b> What is CORTX? CORTX is a distributed object storage system designed for great efficiency, massive capacity, and high HDD-utilization. CORTX is 100% Open Source. </b>

## What our integration does?

Our integration is simple, it allows you to import bulk data from your S3 bucket and once you are with data annotations, you can export it in most widely used annotation formats: JSON, CSV, COCO, PASCAL VOC. Moreover, you can also download the exported data inany Machine Learning environment using these codes.

<img src="https://github.com/vilaksh01/cortx/blob/main/doc/integrations/label-studioAPI/Images/working.jpg">

### Why this integration is important?

Building an AI or ML model that acts like a human requires large volumes of training data. For a model to make decisions and take action, it must be trained to understand specific information. Data annotation is the categorization and labeling of data for AI applications. However, no sole company relies upon one AI dataset or model for it's long term operation, it uses tons of those data in variey of usecases. In order to have a connected system where data annotations and data is stored requires massive scalability and efficiency, completely open source too. With our integration, new startups and AI companeis and store and fetch huge loads of big data from Cortx S3, from making self-driving cars, nano surgical bots and Mars Rovers, all require it.

## Integration walkthrough

Step 1: Download reuirements
- We are integrating S3 storage on label Studio, open source annotation tool, download it using `pip` command:
<pre>
 $ pip install -U label-studio
</pre>
- Start the label studio to verify installation, it would run on local host
<pre>
 $ label-studio 
</pre>

Step 2: CORTX Cloudshare VM lab setup
- Once the Cortex VM is ready, run this command, keep note of external address to that will serve as our S3 endpoint URL
<pre>
 $ sudo route add default gw 192.168.2.1 ens33
 
 <img src="https://github.com/vilaksh01/cortx/blob/main/doc/integrations/label-studioAPI/Images/vmCortx.png">
</pre>
- Start Windows Server 2019 Edition, go to this S3 cortex dashboard, by default S3 user and test bucket is already made for you, a txt file on Desktop contains all default S3 credentials for use 
<pre>
 https://192.168.2.102:28100/#/dashboard
</pre>

Step 3: Connecting S3 data enpoint class and methods for uploading and downloading of data from S3 bucket to anywhere. Our S3DataEndpoint file link: <a href="https://github.com/vilaksh01/cortx/blob/main/doc/integrations/label-studioAPI/s3dataEndpoint.py">S3DataEndpoint Python Class</a> 
```python
   class S3DataEndpoint:
    def __init__(self, end_url, accessKey, secretKey):
        self.end_url = end_url
        self.accessKey = accessKey
        self.secretKey = secretKey

        self.s3_resource = boto3.resource('s3', endpoint_url=self.end_url,
                                          aws_access_key_id=self.accessKey,
                                          aws_secret_access_key=self.secretKey,
                                          config=Config(signature_version='s3v4'),
                                          region_name='US')

        # command to access data from default session
        self.s3_client = boto3.client('s3', endpoint_url=self.end_url,
                                      aws_access_key_id=self.accessKey,
                                      aws_secret_access_key=self.secretKey,
                                      config=Config(signature_version='s3v4'),
                                      region_name='US')
                                      
        # Functions for buckets operation
        def create_bucket_op(self, bucket_name, region):
        if region is None:
            self.s3_client.create_bucket(Bucket=bucket_name)
        else:
            location = {'LocationConstraint': region}
            self.s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration=location)
    
   # dummy test                                 
   def main():
    END_POINT_URL = 'http://uvo100ebn7cuuq50c0t.vm.cld.sr'
    A_KEY = 'AKIAtEpiGWUcQIelPRlD1Pi6xQ'
    S_KEY = 'YNV6xS8lXnCTGSy1x2vGkmGnmdJbZSapNXaSaRhK'

    s3 = S3DataEndpoint(end_url=END_POINT_URL, accessKey=A_KEY, secretKey=S_KEY)
    s3.create_bucket_op('newbucketworks')
    
   if __name__ == "__main__":
    main()
```
If all credentials are correct, you will have a new bucket made which you can see it using CyberDuck on the CloudShare Widnows Server 2019 Edition VM. See below image, our new bucket is created.
<img src="https://github.com/vilaksh01/cortx/blob/main/doc/integrations/label-studioAPI/Images/newBucketCreate.png">

