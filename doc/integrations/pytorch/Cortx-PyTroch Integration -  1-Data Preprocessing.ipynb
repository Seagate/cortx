{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf6af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import zipfile\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "263e78e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\",\n",
    "                    endpoint_url = \"http://192.168.0.29\",\n",
    "                    aws_access_key_id=\"AKIAPo19vPR_TJaeVgleCiOSUw\",\n",
    "                    aws_secret_access_key=\"7cSWM1KCXvRpK4ICeDEAfuicEm+QQeuhqOi7cejZ\",\n",
    "                    region_name = 'eu-central-1',\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99bc37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\",\n",
    "                    endpoint_url = \"http://192.168.0.29\",\n",
    "                    aws_access_key_id=\"AKIAPo19vPR_TJaeVgleCiOSUw\",\n",
    "                    aws_secret_access_key=\"7cSWM1KCXvRpK4ICeDEAfuicEm+QQeuhqOi7cejZ\",\n",
    "                   )\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47c3c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### create bucket\n",
    "def create_bucket(bucket_name, region=None):\n",
    "    if not s3.Bucket(bucket_name) in s3.buckets.all():\n",
    "        if region is None:\n",
    "            s3_client.create_bucket(Bucket=bucket_name)\n",
    "        else:\n",
    "            location = {'LocationConstraint': region}\n",
    "            s3_client.create_bucket(Bucket=bucket_name,\n",
    "                                    CreateBucketConfiguration=location)\n",
    "    else:\n",
    "        print(\"Bucket name already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3dc87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## list all buckets in the s3 a/c\n",
    "def list_bucket():\n",
    "    buckets_list = s3_client.list_buckets()\n",
    "    for bucket in buckets_list['Buckets']:\n",
    "        print(bucket)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f9a73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete s3 bucket\n",
    "def delete_bucket(bucket_name):\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    for key in bucket.objects.all():\n",
    "        key.delete()\n",
    "    bucket.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61bbfb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    if s3_client.get_object(Bucket = bucket, Key = file_name):\n",
    "        print(\"File already uploaded\")\n",
    "    else:\n",
    "        if object_name is None:\n",
    "            object_name = file_name\n",
    "\n",
    "        # Upload the file\n",
    "        try:\n",
    "            response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "        except ClientError as e:\n",
    "            logging.error(e)\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4704ed6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket name already exists\n"
     ]
    }
   ],
   "source": [
    "create_bucket(\"saved-models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f920c237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'agromodel', 'CreationDate': datetime.datetime(2021, 4, 22, 8, 37, 13, tzinfo=tzutc())}\n",
      "{'Name': 'sample-dataset', 'CreationDate': datetime.datetime(2021, 4, 24, 7, 44, 47, tzinfo=tzutc())}\n",
      "{'Name': 'saved-models', 'CreationDate': datetime.datetime(2021, 4, 26, 14, 1, 40, tzinfo=tzutc())}\n",
      "{'Name': 'split-dataset', 'CreationDate': datetime.datetime(2021, 4, 23, 21, 8, 10, tzinfo=tzutc())}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a233445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already uploaded\n"
     ]
    }
   ],
   "source": [
    "upload_file(\"sample_data.zip\", \"sample-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df9f16",
   "metadata": {},
   "source": [
    "#### Unzip the dataset on s3\n",
    "- Unzip sample_data.zip contents to the  Cortx-s3 bucket.\n",
    "\n",
    "- I have already run this process previously, therefore going to skip....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dada266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'sample-dataset'\n",
    "\n",
    "prefix      = \"sample_data.zip\"\n",
    "zipped_keys =  s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "file_list = []\n",
    "for key in zipped_keys['Contents']:\n",
    "    file_list.append(key['Key'])\n",
    "\n",
    "zip_obj = s3.Object(bucket_name=bucket, key=file_list[0])\n",
    "buffer = BytesIO(zip_obj.get()[\"Body\"].read())\n",
    "\n",
    "\n",
    "z = zipfile.ZipFile(buffer)\n",
    "for filename in z.namelist():\n",
    "    file_info = z.getinfo(filename)\n",
    "    s3.meta.client.upload_fileobj(\n",
    "        z.open(filename),\n",
    "        Bucket=bucket,\n",
    "        Key=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5683a",
   "metadata": {},
   "source": [
    "#### Split the data into train and validation sets\n",
    "- Run this ONLY when copyobject is implemeted on Cortx-S3\n",
    "\n",
    "- The initial idea was to upload the image dataset and then split it on the Cortx-S3 using the methods below. However, some functions like move and copy file are not implemented on the current build of s3. Therefore I choose to upload the dataset that is already split. In future you can choose to use the method below to the split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc0995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "from os import path\n",
    "\n",
    "train_path = \"s3://agromodel/Plant_leave_diseases_dataset_without_augmentation/\"\n",
    "def list_dirs(directory):\n",
    "    \"\"\"Returns all directories in a train directory\n",
    "    \"\"\"\n",
    "    return [folder[\"name\"] for folder in client.listdir(directory)[1:]]\n",
    "\n",
    "\n",
    "def list_files(directory):\n",
    "    \"\"\"Returns all the files in a directory\n",
    "    \"\"\"\n",
    "    return [file for file in client.ls(directory)]\n",
    "\n",
    "\n",
    "def split_ratio(input_data, seed, ratio=(0.8, 0.2)):\n",
    "    assert sum(ratio) == 1\n",
    "    for cls_folder in list_dirs(input_data):\n",
    "        split_cls_directory(cls_folder, ratio, seed)\n",
    "\n",
    "\n",
    "def split_cls_directory(directory, ratio, seed):\n",
    "    random.seed(seed)\n",
    "    files = list_files(directory)\n",
    "    random.shuffle(files)\n",
    "\n",
    "    split_train = int(ratio[0] * len(files))\n",
    "\n",
    "    data = split_files(files, split_train)\n",
    "    copy_files(data, directory)\n",
    "\n",
    "\n",
    "def split_files(files, split_train):\n",
    "    files_train = files[:split_train]\n",
    "    files_val = files[split_train:]\n",
    "\n",
    "    data = [(files_train, 'agromodel/train'), (files_val, 'agromodel/val')]\n",
    "    return data\n",
    "\n",
    "\n",
    "def copy_files(data, directory):\n",
    "    \"\"\"Copy the files to the respective folders\"\"\"\n",
    "    class_name = path.split(directory)[1]\n",
    "    for (files, data_type) in data:\n",
    "        print(data_type)\n",
    "        full_path = path.join(data_type, class_name)\n",
    "        print(\"path\", full_path)\n",
    "        client.mkdir(\"s3://\" + full_path, exist_ok=True)\n",
    "        for file in files:\n",
    "            client.copy(file, \"s3://\" + full_path)\n",
    "\n",
    "\n",
    "x = split_ratio(train_path, ratio=(0.8, 0.2), seed=42)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
