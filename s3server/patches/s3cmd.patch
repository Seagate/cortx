diff --git a/S3/Config.py b/S3/Config.py
index eaf495d..e5dc0ec 100644
--- a/S3/Config.py
+++ b/S3/Config.py
@@ -130,6 +130,8 @@ class Config(object):
     content_disposition = None
     content_type = None
     stats = False
+    # Maximum attempts of re-issuing failed requests
+    max_retries = 5
 
     ## Creating a singleton
     def __new__(self, configfile = None, access_key=None, secret_key=None):
diff --git a/S3/S3.py b/S3/S3.py
index 12173ee..d3a11f7 100644
--- a/S3/S3.py
+++ b/S3/S3.py
@@ -242,13 +242,11 @@ class S3(object):
     ## S3 sometimes sends HTTP-307 response
     redir_map = {}
 
-    ## Maximum attempts of re-issuing failed requests
-    _max_retries = 5
-
     def __init__(self, config):
         self.config = config
         self.fallback_to_signature_v2 = False
         self.endpoint_requires_signature_v4 = False
+        self._max_retries = self.config.max_retries
 
     def storage_class(self):
         # Note - you cannot specify GLACIER here
@@ -1073,7 +1071,9 @@ class S3(object):
 
         raise S3Error(response)
 
-    def send_request(self, request, retries = _max_retries):
+    def send_request(self, request, retries = None):
+        if retries is None:
+            retries = self._max_retries
         method_string, resource, headers = request.get_triplet()
 
         debug("Processing request, please wait...")
@@ -1149,7 +1149,9 @@ class S3(object):
 
         return response
 
-    def send_file(self, request, file, labels, buffer = '', throttle = 0, retries = _max_retries, offset = 0, chunk_size = -1):
+    def send_file(self, request, file, labels, buffer = '', throttle = 0, retries = None, offset = 0, chunk_size = -1):
+        if retries is None:
+            retries = self._max_retries
         method_string, resource, headers = request.get_triplet()
         if S3Request.region_map.get(request.resource['bucket'], Config().bucket_location) is None:
             s3_uri = S3Uri(u's3://' + request.resource['bucket'])
@@ -1331,7 +1333,9 @@ class S3(object):
             raise S3UploadError(getTextFromXml(response["data"], 'Message'))
         return response
 
-    def recv_file(self, request, stream, labels, start_position = 0, retries = _max_retries):
+    def recv_file(self, request, stream, labels, start_position = 0, retries = None):
+        if retries is None:
+            retries = self._max_retries
         method_string, resource, headers = request.get_triplet()
         filename = unicodise(stream.name)
         if self.config.progress_meter:
diff --git a/s3cmd b/s3cmd
index d6aa994..6ec1985 100755
--- a/s3cmd
+++ b/s3cmd
@@ -2588,6 +2588,7 @@ def main():
     optparser.add_option(      "--enable", dest="enable", action="store_true", help="Enable given CloudFront distribution (only for [cfmodify] command)")
     optparser.add_option(      "--disable", dest="enable", action="store_false", help="Enable given CloudFront distribution (only for [cfmodify] command)")
     optparser.add_option(      "--cf-invalidate", dest="invalidate_on_cf", action="store_true", help="Invalidate the uploaded filed in CloudFront. Also see [cfinval] command.")
+    optparser.add_option(      "--max-retries", dest="max_retries", type="int", action="store", help="Set maximum retries on failure")
     # joseprio: adding options to invalidate the default index and the default
     # index root
     optparser.add_option(      "--cf-invalidate-default-index", dest="invalidate_default_index_on_cf", action="store_true", help="When using Custom Origin and S3 static website, invalidate the default index file.")
