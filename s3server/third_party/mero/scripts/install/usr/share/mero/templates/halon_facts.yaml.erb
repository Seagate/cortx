<%- # This template's origin resides in http://es-gerrit.xyus.xyratex.com:8080/castor_puppet_modules repository -%>
<%- # It can be found under 'modules/stx_halon/templates/halon_facts.yaml.erb' path -%>
<%- # Current version has been simplified to work w/o puppetdb and is targeted for a singlenode configuration -%>
---
id_sites:
  - site_idx: 0
    site_racks:
<%- client_rack = 0 -%>
<%- q_dbfacts.values.map { |x| x['rack'] }.sort.uniq.select{ |x| !x.nil? }.each do |racknum| -%>
      - rack_idx: <%= racknum %>
        rack_enclosures:
 <%- q_dbfacts.values.sort_by { |j| j['hostname'] }.uniq.each do |dbfact| -%>
       <%- if racknum == dbfact['rack'] -%>
          - enc_idx: <%= dbfact['vpos'] %>
            enc_id: "<%= dbfact['serial_no'] %>"
            enc_bmc:
              - bmc_addr: "<%= dbfact['bmc_ip'] %>"
                bmc_user: "admin"
                bmc_pass: "admin"
              - bmc_addr: "<%= dbfact['bmc2_ip'] %>"
                bmc_user: "admin"
                bmc_pass: "admin"
            enc_hosts:
              - h_fqdn: "<%= dbfact['fqdn'] %>"
                h_memsize: <%= dbfact['memorysize_mb'] %>
                h_cpucount: <%= dbfact['processorcount'] %>
                h_interfaces:
                  - if_macAddress: "<%= dbfact['vlan_manage_macaddress'] %>"
                    if_network: Management
                    if_ipAddrs: [<%= dbfact['vlan_manage_ip'] %>]
                  - if_macAddress: "<%= dbfact['vlan_data_macaddress'] %>"
                    if_network: Data
                    if_ipAddrs: [<%= dbfact['vlan_data_ip'] -%>]
                h_halon:
                  address: "<%= dbfact['vlan_manage_ip'] -%>:<%= halon_port %>"
                  roles:
     <%- if dbfact['role'].include? 'station' -%>
                    - name: station
     <%- end -%>
     <%- if dbfact['role'].include? 'storage' -%>
                    - name: ssu
     <%- end -%>
     <%- if !(dbfact['role'] & ['datamover', 's3server', 'clovis-app']).empty? -%>
                    - name: dm
     <%- end -%>
   <%- end -%>
  <%- end -%>
  <%- client_rack = racknum.to_i + 1 -%>
<%- end -%>
<%- if extra_clients and !extra_clients.empty? -%>
 <%- count = 1 -%>
      - rack_idx: <%= client_rack %>
        rack_enclosures:
 <%- extra_clients.each do |x| -%>
          - enc_idx: <%= count %>
            enc_id: "<%= count %>"
            enc_bmc:
              - bmc_addr: ""
                bmc_user: "admin"
                bmc_pass: "admin"
              - bmc_addr: ""
                bmc_user: "admin"
                bmc_pass: "admin"
            enc_hosts:
              - h_fqdn: "<%= x['m0h_fqdn'] %>"
                h_memsize: <%= x['host_mem_rss'] %>
                h_cpucount: <%= x['host_cores'].length %>
                h_interfaces:
                  - if_macAddress: ""
                    if_network: Management
                    if_ipAddrs: [""]
                  - if_macAddress: ""
                    if_network: Data
                    if_ipAddrs: [<%= x['lnid'].split('@').first %>]
   <%- count += 1 -%>
 <%- end -%>
<%- end -%>
id_m0_servers:
<%- q_dbfacts.values.sort_by { |j| j['hostname'] }.uniq.each do |dbfact| -%>
  <%- cpuarr = []; dbfact['processorcount'].to_i.times do |cpu| cpuarr.push(1); end -%>
  - m0h_fqdn: "<%= dbfact['fqdn'] %>"
    host_mem_as: <%= m0_be_seg_size %>
    host_mem_rss: <%= dbfact['memorysize_mb'].to_i * 1024 %>
    host_mem_stack: <%= dbfact['memorysize_mb'].to_i * 1024 %>
    host_mem_memlock: <%= dbfact['memorysize_mb'].to_i * 1024 %>
    host_cores: <%= cpuarr %>
    lnid: "<%= dbfact['vlan_data_ip'] -%>@<%= m0_lnet_transport -%>"
    m0h_roles:
      - name: ha
  <%- if dbfact['role'].include? 'confd' -%>
      - name: confd
  <%- end -%>
  <%- if dbfact['role'].include? 'storage' -%>
      - name: storage
  <%- end -%>
  <%- if dbfact['role'].include? 'datamover' -%>
      - name: m0t1fs
  <%- end -%>
  <%- if dbfact['role'].include? 'clovis-app' -%>
      - name: clovis-app
  <%- end -%>
  <%- if dbfact['role'].include? 's3server' -%>
      - name: s3server
  <%- end -%>
    <%- if !ios_hash[ dbfact['hostname'] ].empty? -%>
    m0h_devices:
      <%- ios_hash[ dbfact['hostname'] ].each do |key| -%>
      - m0d_wwn: "<%= key['wwn'] %>"
        m0d_serial: "<%= key['serial'] %>"
        m0d_bsize: <%= m0_block_size %>
        m0d_size: <%= key['size'] %>
        m0d_path: "<%= key['path'] %>"
        m0d_slot: <%= key['bay'] %>
      <%- end -%>
    <%- else -%>
    m0h_devices: []
    <%- end -%>
<%- end -%>
<%- if extra_clients and !extra_clients.empty? -%>
  <%= extra_clients.to_yaml.gsub(/---\s*\n\s*/, '') + "\n" -%>
<%- end -%>
id_m0_globals:
  m0_be_ios_seg_size: <%= m0_be_ios_seg_size %>
  m0_be_log_size: <%= m0_be_log_size %>
  m0_be_seg_size: <%= m0_be_seg_size %>
  m0_be_tx_payload_size_max: <%= m0_be_tx_payload_size_max %>
  m0_be_tx_reg_nr_max: <%= m0_be_tx_reg_nr_max %>
  m0_be_tx_reg_size_max: <%= m0_be_tx_reg_size_max %>
  m0_be_txgr_freeze_timeout_max: <%= m0_be_txgr_freeze_timeout_max %>
  m0_be_txgr_freeze_timeout_min: <%= m0_be_txgr_freeze_timeout_min %>
  m0_be_txgr_payload_size_max: <%= m0_be_txgr_payload_size_max %>
  m0_be_txgr_reg_nr_max: <%= m0_be_txgr_reg_nr_max %>
  m0_be_txgr_reg_size_max: <%= m0_be_txgr_reg_size_max %>
  m0_be_txgr_tx_nr_max: <%= m0_be_txgr_tx_nr_max %>
  m0_md_redundancy: <%= m0_md_redundancy %>
  m0_min_rpc_recvq_len: <%= m0_min_rpc_recvq_len %>
id_pools:
  <%- pools.each do |pool| -%>
  - pool_id: <%= pool[:name] %>
    pool_pdclust_attrs:
      data_units: <%= pool[:data_units] %>
      parity_units: <%= pool[:parity_units] %>
      unit_size: <%= m0_block_size %>
      seed: [101, 102]
    pool_allowed_failures: <%- if pool[:allowed_failures] == [] -%>[]<%- end %>
      <%- pool[:allowed_failures].each do |v| -%>
      - {site: <%= v[0] -%>, rack: <%= v[1] -%>, encl: <%= v[2] -%>, ctrl: <%= v[3] -%>, disk: <%= v[4] -%>}
      <%- end -%>
<%- if false -%>
# For parity 2 we can tolerate 2 disk failures:
#      - {site: 0, rack: 0, encl: 0, ctrl: 0, disk: 2}
# For 8+2 layout (8 data units + 2 parity units in group)
# and 6+ nodes configuration we can tolerate the failure
# of the whole node:
#      - {site: 0, rack: 0, encl: 0, ctrl: 1, disk: 0}
# And in case of 12+ nodes (8+2 layout), 1 node +1 disk:
#      - {site: 0, rack: 0, encl: 0, ctrl: 1, disk: 1}
<%- end -%>
    pool_device_refs:
      <%- pool[:disks].each do |disk| -%>
        <%- if disk['is_fake'] -%>
      - dr_path: "<%= disk['path'] %>"
        dr_serial: "<%= disk['serial'] %>"
        <%- else -%>
      - dr_wwn: "<%= disk['wwn'] %>"
        dr_serial: "<%= disk['serial'] %>"
        <%- end -%>
      <%- end -%>
  <%- end -%>
id_profiles:
  - prof_id: prof-0
    prof_pools:
      <%- pools.each do |pool| -%>
      - <%= pool[:name] %>
      <%- end -%>
