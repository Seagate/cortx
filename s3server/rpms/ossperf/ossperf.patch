diff --git a/ossperf.sh b/ossperf.sh
index 1009bf5..ec5466a 100755
--- a/ossperf.sh
+++ b/ossperf.sh
@@ -31,15 +31,15 @@ command -v ping >/dev/null 2>&1 || { echo >&2 "ossperf requires the command line
 
 function usage
 {
-echo "$SCRIPT -n files -s size [-b <bucket>] [-u] [-a] [-m <alias>] [-z] [-g] [-k] [-p] [-o]
-
+echo "$SCRIPT -n files -s size [-b <bucket>] [-c <config file>] [-u] [-a] [-m <alias>] [-z] [-g] [-k] [-p] [-o] [-x]
 This script analyzes the performance and data integrity of S3-compatible
-storage services 
+storage services
 
 Arguments:
 -h : show this message on screen
 -n : number of files to be created
--s : size of the files to be created in bytes (max 16777216 = 16 MB)
+-s : size of the files to be created in bytes
+-c : configuration file for s3cmd
 -b : ossperf will create per default a new bucket ossperf-testbucket (or OSSPERF-TESTBUCKET, in case the argument -u is set). This is not a problem when private cloud deployments are investigated, but for public cloud scenarios it may become a problem, because object-based stoage services implement a global bucket namespace. This means that all bucket names must be unique. With the argument -b <bucket> the users of ossperf have the freedom to specify the bucket name
 -u : use upper-case letters for the bucket name (this is required for Nimbus Cumulus and S3ninja)
 -a : use the Swift API and not the S3 API (this requires the python client for the Swift API and the environment variables ST_AUTH, ST_USER and ST_KEY)
@@ -49,6 +49,7 @@ Arguments:
 -k : keep the local files and the directory afterwards (do not clean up)
 -p : upload and download the files in parallel
 -o : appended the results to a local file results.csv
+-x : use http connection instead of https
 "
 exit 0
 }
@@ -67,6 +68,8 @@ NOT_CLEAN_UP=0
 PARALLEL=0
 LIST_OF_FILES=
 OUTPUT_FILE=0
+S3CMD_CONFIG=""
+USE_HTTP=""
 
 RED='\033[0;31m'          # Red color
 NC='\033[0m'              # No color
@@ -75,23 +78,26 @@ YELLOW='\033[0;33m'       # Yellow color
 BLUE='\033[0;34m'         # Blue color
 WHITE='\033[0;37m'        # White color
 
-while getopts "hn:s:b:uamzgkpo" Arg ; do
+while getopts "hn:s:b:c:uamzgkpox" Arg ; do
   case $Arg in
     h) usage ;;
     n) NUM_FILES=$OPTARG ;;
     s) SIZE_FILES=$OPTARG ;;
     # If the flag has been set => $NOT_CLEAN_UP gets value 1
     b) BUCKETNAME_PARAMETER=1
-       BUCKET=$OPTARG ;; 
+       BUCKET=$OPTARG ;;
+    c) CONFIGFILE=1
+       CONFIG_FILENAME=$OPTARG ;;
     u) UPPERCASE=1 ;;
     a) SWIFT_API=1 ;;
-    m) MINIO_CLIENT=1 
+    m) MINIO_CLIENT=1
        MINIO_CLIENT_ALIAS=$OPTARG ;;
     z) AZURE_CLI=1 ;;
     g) GOOGLE_API=1 ;;
     k) NOT_CLEAN_UP=1 ;;
     p) PARALLEL=1 ;;
     o) OUTPUT_FILE=1 ;;
+    x) USE_HTTP="--no-ssl"; shift ;;
     \?) echo "Invalid option: $OPTARG" >&2
        exit 1
        ;;
@@ -99,6 +105,10 @@ while getopts "hn:s:b:uamzgkpo" Arg ; do
 done
 
 
+if [ "$CONFIGFILE" -eq 1 ] ; then
+   S3CMD_CONFIG="-c $CONFIG_FILENAME"
+fi
+
 # Only if the user wants to execute the upload and dowload of the files in parallel...
 if [ "$PARALLEL" -eq 1 ] ; then
   # ... the script needs to check, if the command line tool GNU parallel is installed
@@ -191,38 +201,36 @@ if [ "$NUM_FILES" -eq 0 ] ; then
   exit 1
 fi
 
+# Commented the file size restriction
 # Validate that...
 # SIZE_FILES is not 0 and not bigger than 16777216
-if ( [[ "$SIZE_FILES" -eq 0 ]] || [[ "$SIZE_FILES" -gt 16777216 ]] ) ; then
-   echo -e "${RED}Attention: The size of the file(s) must not 0 and the maximum size is 16.777.216 Byte!${NC}"
-   usage
-   exit 1
-fi
- 
+#if ( [[ "$SIZE_FILES" -eq 0 ]] || [[ "$SIZE_FILES" -gt 16777216 ]] ) ; then
+#   echo -e "${RED}Attention: The size of the file(s) must not 0 and the maximum size is 16.777.216 Byte!${NC}"
+#   usage
+#   exit 1
+#fi
  
+# Commented the internet check, not needed for S3server
 # We shall check at least 5 times
-LOOP_VARIABLE=5  
+#LOOP_VARIABLE=5  
 #until LOOP_VARIABLE is greater than 0 
-while [ $LOOP_VARIABLE -gt "0" ]; do 
+#while [ $LOOP_VARIABLE -gt "0" ]; do 
   # Check if we have a working network connection by sending a ping to 8.8.8.8
-  if ping -q -c 1 -W 1 8.8.8.8 >/dev/null ; then
-    echo -e "${GREEN}[OK] This computer has a working internet connection.${NC}"
+#  if ping -q -c 1 -W 1 8.8.8.8 >/dev/null ; then
+#    echo -e "${GREEN}[OK] This computer has a working internet connection.${NC}"
     # Skip entire rest of loop.
-    break
-  else
-    echo -e "${YELLOW}[INFO] The internet connection is not working now. Will check again.${NC}"
+#    break
+#  else
+#    echo -e "${YELLOW}[INFO] The internet connection is not working now. Will check again.${NC}"
     # Decrement variable
-    LOOP_VARIABLE=$((LOOP_VARIABLE-1))
-    if [ "LOOP_VARIABLE" -eq 0 ] ; then
-      echo -e "${RED}[ERROR] This computer has no working internet connection. Please check your network settings.${NC}" && exit 1
-    fi
-    # Wait a moment. 
-    sleep 1
-  fi
-done
-
-
-
+#    LOOP_VARIABLE=$((LOOP_VARIABLE-1))
+#    if [ "LOOP_VARIABLE" -eq 0 ] ; then
+#      echo -e "${RED}[ERROR] This computer has no working internet connection. Please check your network settings.${NC}" && exit 1
+#    fi
+    # Wait a moment.
+#    sleep 1
+#  fi
+#done
 
 # Check if the directory already exists
 # This is not a part of the benchmark!
@@ -296,7 +304,7 @@ elif [ "$GOOGLE_API" -eq 1 ] ; then
   fi
 else
   # use the S3 API with s3cmd
-  if s3cmd mb s3://$BUCKET ; then
+  if s3cmd $S3CMD_CONFIG $USE_HTTP mb s3://$BUCKET ; then
     echo -e "${GREEN}[OK] Bucket ${BUCKET} has been created.${NC}"
   else
     echo -e "${RED}[ERROR] Unable to create the bucket ${BUCKET}.${NC}" && exit 1
@@ -323,10 +331,10 @@ sleep 1
 if [ "$SWIFT_API" -ne 1 ] ; then
   # We shall check at least 5 times
   LOOP_VARIABLE=5
-  # until LOOP_VARIABLE is greater than 0 
-  while [ $LOOP_VARIABLE -gt "0" ]; do 
+  # until LOOP_VARIABLE is greater than 0
+  while [ $LOOP_VARIABLE -gt "0" ]; do
     # Check if the Bucket is accessible
-    if s3cmd ls s3://$BUCKET ; then
+    if s3cmd $S3CMD_CONFIG ls $USE_HTTP s3://$BUCKET ; then
       echo -e "${GREEN}[OK] The bucket is available.${NC}"
       # Skip entire rest of loop.
       break
@@ -389,7 +397,7 @@ if [ "$PARALLEL" -eq 1 ] ; then
   else
   # use the S3 API with s3cmd
     # Upload files in parallel
-    if find $DIRECTORY/*.txt | parallel s3cmd put {} s3://$BUCKET ; then
+    if find $DIRECTORY/*.txt | parallel s3cmd $S3CMD_CONFIG put {} $USE_HTTP s3://$BUCKET ; then
       echo -e "${GREEN}[OK] Files have been uploaded.${NC}"
     else
       echo -e "${RED}[ERROR] Unable to upload the files.${NC}" && exit 1
@@ -434,7 +442,7 @@ else
   else
   # use the S3 API with s3cmd
     # Upload files sequentially
-    if s3cmd put $DIRECTORY/*.txt s3://$BUCKET ; then
+    if s3cmd $S3CMD_CONFIG put $DIRECTORY/*.txt $USE_HTTP s3://$BUCKET ; then
       echo -e "${GREEN}[OK] Files have been uploaded.${NC}"
     else
       echo -e "${RED}[ERROR] Unable to upload the files.${NC}" && exit 1
@@ -503,7 +511,7 @@ elif [ "$GOOGLE_API" -eq 1 ] ; then
   fi
 else
   # use the S3 API with s3cmd
-  if s3cmd ls s3://$BUCKET ; then
+  if s3cmd $S3CMD_CONFIG ls $USE_HTTP s3://$BUCKET ; then
     echo -e "${GREEN}[OK] The list of objects inside ${BUCKET} has been fetched.${NC}"
   else
     echo -e "${RED}[ERROR] Unable to fetch the list of objects inside ${BUCKET}.${NC}" && exit 1
@@ -568,7 +576,7 @@ if [ "$PARALLEL" -eq 1 ] ; then
   else
   # use the S3 API with s3cmd
     # Download files in parallel
-    if find ${DIRECTORY}/*.txt -type f -printf "%f\n" | parallel s3cmd get --force s3://$BUCKET/{} $DIRECTORY/ ; then
+    if find ${DIRECTORY}/*.txt -type f -printf "%f\n" | parallel s3cmd $S3CMD_CONFIG get --force $USE_HTTP s3://$BUCKET/{} $DIRECTORY/ ; then
       echo -e "${GREEN}[OK] Files have been downloaded.${NC}"
     else
       echo -e "${RED}[ERROR] Unable to download the files.${NC}" && exit 1
@@ -616,7 +624,7 @@ else
   else
   # use the S3 API with s3cmd
     # Download files sequentially
-    if s3cmd get --force s3://$BUCKET/*.txt $DIRECTORY/ ; then
+    if s3cmd $S3CMD_CONFIG get --force $USE_HTTP s3://$BUCKET/*.txt $DIRECTORY/ ; then
       echo -e "${GREEN}[OK] Files have been downloaded.${NC}"
     else
       echo -e "${RED}[ERROR] Unable to download the files.${NC}" && exit 1
@@ -702,7 +710,7 @@ if [ "$PARALLEL" -eq 1 ] ; then
   else
   # use the S3 API with s3cmd
     #  Erase files (objects) inside the bucket in parallel
-    if find $DIRECTORY/*.txt | parallel s3cmd del --recursive s3://$BUCKET/* ; then
+    if find $DIRECTORY/*.txt -type f -printf "%f\n" | parallel s3cmd $S3CMD_CONFIG del $USE_HTTP s3://$BUCKET/{} ; then
       echo -e "${GREEN}[OK] Files inside the bucket ${BUCKET} have been erased.${NC}"
     else
       echo -e "${RED}[ERROR] Unable to erase the files inside the bucket ${BUCKET}.${NC}" && exit 1
@@ -747,7 +755,7 @@ else
   else
   # use the S3 API with s3cmd
     # Erase files (objects) inside the bucket sequentially
-    if s3cmd del s3://$BUCKET/* ; then
+    if s3cmd $S3CMD_CONFIG del $USE_HTTP s3://$BUCKET/* ; then
       echo -e "${GREEN}[OK] Files inside the bucket ${BUCKET} have been erased.${NC}"
     else
       echo -e "${RED}[ERROR] Unable to erase the files inside the bucket ${BUCKET}.${NC}" && exit 1
@@ -844,7 +852,7 @@ elif [ "$GOOGLE_API" -eq 1 ] ; then
   fi
 else
   # use the S3 API with s3cmd
-  if s3cmd rb --force --recursive s3://$BUCKET ; then
+  if s3cmd $S3CMD_CONFIG rb --force --recursive $USE_HTTP s3://$BUCKET ; then
     echo -e "${GREEN}[OK] Bucket ${BUCKET} has been erased.${NC}"
   else
     echo -e "${RED}[ERROR] Unable to erase the bucket ${BUCKET}.${NC}" && exit 1
