diff --git a/S3/Config.py b/S3/Config.py
index e6e76f0..66741b9 100644
--- a/S3/Config.py
+++ b/S3/Config.py
@@ -198,6 +198,8 @@ class Config(object):
     content_disposition = u""
     content_type = u""
     stats = False
+    # Maximum attempts of re-issuing failed requests
+    max_retries = 5
     # Disabled by default because can create a latency with a CONTINUE status reply
     # expected for every send file requests.
     use_http_expect = False
diff --git a/S3/S3.py b/S3/S3.py
index 8e454e4..3eff999 100644
--- a/S3/S3.py
+++ b/S3/S3.py
@@ -246,14 +246,13 @@ class S3(object):
         "BucketAlreadyExists" : "Bucket '%s' already exists",
     }
 
-    ## Maximum attempts of re-issuing failed requests
-    _max_retries = 5
 
     def __init__(self, config):
         self.config = config
         self.fallback_to_signature_v2 = False
         self.endpoint_requires_signature_v4 = False
         self.expect_continue_not_supported = False
+        self._max_retries = self.config.max_retries
 
     def storage_class(self):
         # Note - you cannot specify GLACIER here
@@ -1227,7 +1226,9 @@ class S3(object):
 
         raise S3Error(response)
 
-    def send_request(self, request, retries = _max_retries):
+    def send_request(self, request, retries = None):
+        if retries is None:
+            retries = self._max_retries
         if request.resource.get('bucket') \
            and not request.use_signature_v2() \
            and S3Request.region_map.get(request.resource['bucket'],
@@ -1343,8 +1344,10 @@ class S3(object):
         return response
 
     def send_file(self, request, stream, labels, buffer = '', throttle = 0,
-                  retries = _max_retries, offset = 0, chunk_size = -1,
+                  retries = None, offset = 0, chunk_size = -1,
                   use_expect_continue = None):
+        if retries is None:
+            retries = self._max_retries
         if request.resource.get('bucket') \
            and not request.use_signature_v2() \
            and S3Request.region_map.get(request.resource['bucket'],
@@ -1613,7 +1616,9 @@ class S3(object):
             raise S3UploadError(getTextFromXml(response["data"], 'Message'))
         return response
 
-    def recv_file(self, request, stream, labels, start_position = 0, retries = _max_retries):
+    def recv_file(self, request, stream, labels, start_position = 0, retries = None):
+        if retries is None:
+            retries = self._max_retries
         if request.resource.get('bucket') \
            and not request.use_signature_v2() \
            and S3Request.region_map.get(request.resource['bucket'],
diff --git a/s3cmd b/s3cmd
index 2ef1350..35112f7 100755
--- a/s3cmd
+++ b/s3cmd
@@ -2728,6 +2728,7 @@ def main():
     optparser.add_option(      "--enable", dest="enable", action="store_true", help="Enable given CloudFront distribution (only for [cfmodify] command)")
     optparser.add_option(      "--disable", dest="enable", action="store_false", help="Disable given CloudFront distribution (only for [cfmodify] command)")
     optparser.add_option(      "--cf-invalidate", dest="invalidate_on_cf", action="store_true", help="Invalidate the uploaded filed in CloudFront. Also see [cfinval] command.")
+    optparser.add_option(      "--max-retries", dest="max_retries", type="int", action="store", help="Set maximum retries on failure")
     # joseprio: adding options to invalidate the default index and the default
     # index root
     optparser.add_option(      "--cf-invalidate-default-index", dest="invalidate_default_index_on_cf", action="store_true", help="When using Custom Origin and S3 static website, invalidate the default index file.")
